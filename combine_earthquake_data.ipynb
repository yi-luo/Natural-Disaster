{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Disaster\n",
    "\n",
    "The purpose of this project is to use regression analysis to estimate natural disaster casulties and damage cost. This will be of great value in emergency planning and resource allocation in the event of a natural disaster. \n",
    "\n",
    "**Datasets Used**\n",
    "* *EM-DAT: Natural Disasters Database.*\n",
    "    Historical earthquake, storm and flood data for years 2000-2019\n",
    "\n",
    "* *Global population estimates by longitude and latitude* For years 2000, 2005, 2010, 2015 and 2020(estimate)\n",
    "    \n",
    "* *USGS Earthquake Database.* Historical earthquake catalog with details such as earthquake magnitude, intensity and depth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**References**\n",
    "* Center for International Earth Science Information Network - CIESIN - Columbia University. 2018. Gridded Population of the World, Version 4 (GPWv4): Administrative Unit Center Points with Population Estimates, Revision 11. Palisades, NY: NASA Socioeconomic Data and Applications Center (SEDAC). https://doi.org/10.7927/H4BC3WMT. Accessed June 9, 2019\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "#sc =SparkContext()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data = pd.read_excel(\"earthquake_disaster_info_2000.xlsx\")\n",
    "earthquake_details = pd.read_excel(\"earthquake_details.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pop_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1ecfb012f917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m print(\"Disaster Data: {}, Population Data: {}, Earthquake Date: {}\".format(\n\u001b[0;32m----> 2\u001b[0;31m     earthquake_data.shape, pop_data.shape, earthquake_details.shape))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pop_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Disaster Data: {}, Population Data: {}, Earthquake Date: {}\".format(\n",
    "    earthquake_data.shape, pop_data.shape, earthquake_details.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_details.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data['Country'].value_counts().head(10).plot(kind='bar', figsize=(15,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_magnitude = earthquake_data['Magnitude value'].values\n",
    "sns.set(color_codes=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot(arr_magnitude, bins=20)\n",
    "plt.title(\"Distribution of Magnitude Values (Ritcher Scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between Magnitude Value and Total Death?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_death = earthquake_data['Total deaths'].values\n",
    "sns.jointplot(arr_magnitude, arr_death)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no direct linear correlation between magnitude value and total death. A magnitude value of 9.0+ does not correlate to higher death rate. Why is that? Earthquake damage depends moreso on what area is hit. If an unpopulated region is struck, there will be low loss of life or property. However, if the earthquake is struck in a highly densed area, there will be more casualities. Therefore, magnitude alone cannot be used to predict death rate. Another important factor is location and location population/density. I'll need to somehow incorporate the **Total Affected** data into my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the strength and severity of earthquakes, I need to not only look at the **magnitutde** of an earthquake but also its **intensity** and **depth**. The EM-DAT data sets stores information about the magnitude of the earthquake, but I need to join it with the USGS earthquake datasets to pull the depth and intensity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting earthquake details 'time' column into year and month\n",
    "details_join = earthquake_details.copy()\n",
    "details_join = details_join[['time', 'latitude', 'longitude', 'depth', 'mag','place']]\n",
    "details_join['year'] = details_join['time'].apply(lambda x: x.split('-')[0]).astype('int')\n",
    "details_join['month'] = details_join['time'].apply(lambda x: x.split('-')[1]).astype('int')\n",
    "details_join['day'] = details_join['time'].apply(lambda x: x.split('-')[2][:2]).astype('int')\n",
    "details_join['latitude_round'] = round(details_join['latitude'])\n",
    "details_join['longitude_round'] = round(details_join['longitude'])\n",
    "\n",
    "def split_country(place):\n",
    "    split_list = place.split(', ')\n",
    "    list_len = len(split_list)\n",
    "    if list_len > 1: return split_list[1]\n",
    "    else: return split_list[0]\n",
    "    \n",
    "details_join['country'] = details_join['place'].astype(str).apply(lambda x: split_country(x))\n",
    "remove_cardinal = [\"southeastern \",\"northeastern \",\"southwestern \",\"northwestern \",\n",
    "                   \"western \",\"eastern \",\"southern \",\"northern \",\"near the coast of central \"]\n",
    "for loc in remove_cardinal:\n",
    "    details_join['country'].replace(loc,\"\",regex=True,inplace=True)\n",
    "details_join.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split natural disaster 'Start date' column into year, month, day\n",
    "earthquake_feature_data = earthquake_data[['Start date','ISO','Country','Latitude','Longitude',\n",
    "                                  'Magnitude value','Total deaths']].copy()\n",
    "earthquake_feature_data['Month'] = earthquake_data['Start date'].apply(lambda x: x.split('/')[1]).astype('int')\n",
    "earthquake_feature_data['Year'] = earthquake_data['Start date'].apply(lambda x: x.split('/')[2]).astype('int')\n",
    "earthquake_feature_data['Day'] = earthquake_data['Start date'].apply(lambda x: x.split('/')[0]).astype('int')\n",
    "earthquake_feature_data['Latitude_round'] = round(earthquake_data['Latitude'])\n",
    "earthquake_feature_data['Longitude_round'] = round(earthquake_data['Longitude'])\n",
    "\n",
    "## standardize country names\n",
    "rename_country = [\" \\(the\\)\",\" \\(Islamic Republic of\\)\",\" \\(Province of China\\)\",\n",
    "                  \", United Republic of\",\" \\(the Democratic Republic of the\\)\", \" Federation\"]\n",
    "earthquake_feature_data['Country'].replace(\"Russian Federation\",\"Russia\",regex=True,inplace=True)\n",
    "for country in rename_country:\n",
    "    earthquake_feature_data['Country'].replace(country,\"\",regex=True,inplace=True)\n",
    "\n",
    "print(earthquake_feature_data.info())\n",
    "print(earthquake_feature_data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_feature_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details_join.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_merge_data = earthquake_feature_data.copy()\n",
    "earthquake_merge_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge attempt 1: Exact Date and Location\n",
    "def merge_data(earthquake,details,earth_col,details_col):\n",
    "    details_dist = details.drop_duplicates(subset=details_col)\n",
    "    df_merge = pd.merge(earthquake,details_dist,how='left',\n",
    "                       left_on=earth_col,right_on=details_col)\n",
    "    null = df_merge['mag'].isnull()\n",
    "    print(\"Number of un-merged rows left: {}\".format(null.sum()))\n",
    "    null_index = df_merge[null]['index'].values\n",
    "    print(\"Un-merged index rows: {}\".format(null_index))\n",
    "    full_df=df_merge[~null]\n",
    "    print(\"Merge dataframe shape: {}\".format(full_df.shape))\n",
    "    return full_df,null_index\n",
    "\n",
    "\n",
    "merged_df1, null_index = merge_data(earthquake_merge_data,details_join,\n",
    "                                   ['Latitude_round','Longitude_round','Year','Month','Day'],\n",
    "                                   ['latitude_round','longitude_round','year','month','day'])\n",
    "earthquake_merge_df = earthquake_merge_data.iloc[null_index,:]\n",
    "print(earthquake_merge_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge attempt 2: Exact Date and Country\n",
    "merged_df2, null_index2 = merge_data(earthquake_merge_df,details_join,\n",
    "                                   ['Country','Year','Month','Day'],\n",
    "                                   ['country','year','month','day'])\n",
    "earthquake_merge_df2 = earthquake_merge_data.iloc[null_index2,:]\n",
    "print(earthquake_merge_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge attempt 3: Date -1 and Country\n",
    "earthquake_merge_df2_date_adj = earthquake_merge_df2.copy()\n",
    "earthquake_merge_df2_date_adj['Day'] = earthquake_merge_df2_date_adj['Day'].astype(int) - 1\n",
    "merged_df3, null_index3 = merge_data(earthquake_merge_df2_date_adj,details_join,\n",
    "                                   ['Country','Year','Month','Day'],\n",
    "                                   ['country','year','month','day'])\n",
    "earthquake_merge_df3 = earthquake_merge_data.iloc[null_index3,:]\n",
    "\n",
    "## merge attempt 4: Date +1 and Country\n",
    "earthquake_merge_df3_date_adj = earthquake_merge_df3.copy()\n",
    "earthquake_merge_df3_date_adj['Day'] = earthquake_merge_df3_date_adj['Day'].astype(int) + 1\n",
    "merged_df4, null_index4 = merge_data(earthquake_merge_df3_date_adj,details_join,\n",
    "                                   ['Country','Year','Month','Day'],\n",
    "                                   ['country','year','month','day'])\n",
    "earthquake_merge_df4 = earthquake_merge_data.iloc[null_index4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df5, null_index5 = merge_data(earthquake_merge_df4,details_join,\n",
    "                                   ['Latitude_round','Longitude_round','Year','Month'],\n",
    "                                   ['latitude_round','longitude_round','year','month'])\n",
    "earthquake_merge_df5 = earthquake_merge_data.iloc[null_index5,:]\n",
    "earthquake_merge_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = [28,476,343,352,318,307,161,96,136,151,165,185,190]\n",
    "earthquake_merge_df6 = earthquake_merge_df5[~earthquake_merge_df5.index.isin(drop_rows)]\n",
    "earthquake_merge_df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_merge = [(467,21165),(398,107067)]\n",
    "def join_row(index1,index2):\n",
    "    row1 = earthquake_merge_df6[earthquake_merge_df6.index==index1]\n",
    "    print(row1.shape)\n",
    "    row2 = details_join[details_join.index==index2]\n",
    "    print(row2.shape)\n",
    "    row = row1.join(row2)\n",
    "    print(row)\n",
    "    return row\n",
    "manual_merge_list = [join_row(pair[0],pair[1]) for pair in manual_merge]\n",
    "pd.concat(manual_merge_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([merged_df1,merged_df2,merged_df3,merged_df4,merged_df5])\n",
    "print(merged_df.shape)\n",
    "print(merged_df.columns)\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['diff_mag'] = merged_df['Magnitude value'] - merged_df['mag']\n",
    "merged_df['diff_mag'].plot(kind='hist',figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['diff_mag']>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
